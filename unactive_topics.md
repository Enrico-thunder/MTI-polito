Table of contents
=================
   * [Thesis proposals](#thesis-proposals)
     * [Multimodal Learning](#multimodal-learning-)


Thesis proposals
============

### Multimodal Learning <a href="mailto:moreno.laquatra@polito.it?subject=Multimodal Learning - YOUR NAME HERE"><img src="https://shields.io/badge/-available-green" alt="Available"></a>

<a href="https://en.wikipedia.org/wiki/Natural_language_processing"><img src="https://img.shields.io/badge/NLP-Natural%20Language%20Processing-yellow" alt="Natural Language Processing"></a> 
<a href="https://en.wikipedia.org/wiki/Computer_Vision"><img src="https://img.shields.io/badge/CV-Computer%20Vision-green" alt="Computer Vision"></a>

You can find an explaination of Multimodal learning in videos, schemes, images, podcasts. This is multimodal learning, it consists of several techniques that are able to combine the input from different modalities (images, videos, audio, text, etc.) at _the same time_. 
**Multi** refers to the simultaneous process.
**Modal** refers to the modalities (images, audios, videos, etc.).
**Learning** refers to the automated process involved (machine or deep learning).

The **main objectives** of this thesis are:

- Define and explore the state of the art in Multimodal learning.
- Investigate the deep learning techniques that can be designed or adapted for a multimodal scenario. 
- Simulate the process on existing multimodal benchmark dataset to prove the effectiveness of the designed methodology.

**References :books::**

Hu, R., & Singh, A. (2021). Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer. arXiv preprint arXiv:2102.10772. [article](https://arxiv.org/abs/2102.10772)

Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265. [article](https://arxiv.org/abs/1908.02265)

Lin, X., Bertasius, G., Wang, J., Chang, S. F., Parikh, D., & Torresani, L. (2021). Vx2Text: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs. arXiv preprint arXiv:2101.12059. [article](https://arxiv.org/abs/2101.12059)

**Interesting projects :computer::**
- [List of updated references](https://github.com/pliang279/awesome-multimodal-ml)
- [MMF: multimodal framework](https://mmf.sh/)

**Additional Material:**

- [BERT Can See Out of the Box](https://www.youtube.com/watch?v=K2ho2zoulT4)
- [Learning Visiolinguistic Representations with ViLBERT w/ Stefan Lee - Podcast](https://www.youtube.com/watch?v=LDrQgIhXlk8)
- [Vilbert talk](https://www.youtube.com/watch?v=hFVM1PylYZI) [demo](https://vilbert.cloudcv.org/)
